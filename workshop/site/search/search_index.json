{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GenAI Stack Workshop","text":"<p>The GenAI Stack is a pre-configured, ready-to-code, and secure environment that makes it easy for developers to build and deploy GenAI applications.</p> <p>The GenAI Stack is a collaborative effort launched by Docker, Neo4j, LangChain, and Ollama at DockerCon 2023, aimed at streamlining the development of generative AI applications. This stack integrates several cutting-edge technologies to provide developers with a comprehensive toolkit for building AI-powered applications with ease.</p> <p>Key components of the GenAI Stack include:</p> <ol> <li> <p>Pre-configured Large Language Models (LLMs): The stack comes with pre-configured LLMs like Llama2, GPT-3.5, and GPT-4, enabling developers to kickstart their AI projects quickly.</p> </li> <li> <p>Ollama Management: Ollama facilitates the local management of open-source LLMs, streamlining the AI development process.</p> </li> <li> <p>Neo4j as the Default Database: Neo4j serves as the stack's default database, offering advanced graph and vector search capabilities. This enhances the speed and accuracy of AI/ML models by uncovering complex data patterns and relationships. Additionally, Neo4j acts as a long-term memory for these models, supporting more precise GenAI predictions and outcomes with its knowledge graphs.</p> </li> <li> <p>LangChain Orchestration: LangChain provides a framework for applications powered by LLMs, facilitating communication between the LLM, the application, and the database. It includes LangSmith for debugging, testing, evaluating, and monitoring LLM applications.</p> </li> <li> <p>Comprehensive Support: Docker and its partners offer a variety of tools, code templates, how-to guides, and best practices to support developers in their GenAI journey.</p> </li> </ol> <p>The GenAI Stack is designed to make AI/ML integration more accessible to developers, offering a ready-to-code and secure environment. By bringing together these technologies, the stack eliminates the need for developers to search for, assemble, and configure disparate technologies from different sources. The stack is available in Early Access and can be accessed from the Docker Desktop Learning Center or directly on GitHub.</p> <p>Gen-AI Stack Workshops are a series of workshops designed to teach developers how to build and deploy GenAI applications using the GenAI Stack. These workshops are ideal for developers who are interested in learning more about GenAI or who are looking for a hands-on introduction to the GenAI Stack.</p> <p>These hands-on labs will be a combination of theory and practical exercises.</p>"},{"location":"#github-sources","title":"GitHub Sources","text":"<p>The source code for this workshop is available here.</p>"},{"location":"#co-authors","title":"Co-Authors","text":"<ul> <li>Ajeet Singh Raina - DevRel @Docker</li> <li>Siddhant Agarwal - Developer Relations APAC @Neo4j</li> </ul>"},{"location":"#benefits-of-this-genai-stack-workshop","title":"Benefits of this GenAI Stack Workshop","text":"<ul> <li>Learn about GenAI and why it is important</li> <li>Learn how to use Docker to containerize and deploy GenAI applications</li> <li>Learn how to use Neo4j to store and manage knowledge graphs</li> <li>Learn how to use LangChain to generate text, translate languages, and write different kinds of creative content</li> <li>Learn how to use Ollama to train and deploy large language models</li> <li>Learn how to use the GenAI Stack to build and deploy GenAI applications</li> <li>Get hands-on experience with the GenAI Stack</li> <li>Network with other GenAI developers and trainers</li> </ul>"},{"location":"#applications-and-uses","title":"Applications and Uses","text":""},{"location":"#1-import-and-embed-data-from-stack-overflow-via-tags","title":"1. Import and Embed Data From Stack Overflow via Tags","text":""},{"location":"#2-support-agent-app-query-the-imported-data-via-a-chat-interface-using-vector-graph-search","title":"2. Support Agent App: Query the Imported Data via a Chat Interface Using Vector + Graph Search","text":""},{"location":"#3-generate-new-questions-in-the-style-of-highly-ranked-existing-ones","title":"3. Generate New Questions in the Style of Highly Ranked Existing Ones","text":""},{"location":"#4-read-local-pdf-and-ask-it-questions-fullstack-python-application","title":"4. Read local PDF and ask it questions. Fullstack Python application.","text":""},{"location":"lab1/best-practices/","title":"Best Practices","text":"<p>Generative AI (GenAI) is a rapidly developing field with the potential to revolutionize many industries. However, it is important to use GenAI responsibly and ethically. Here are some best practices for using GenAI:</p>"},{"location":"lab1/best-practices/#1-use-a-variety-of-training-data","title":"1. Use a variety of training data.","text":"<p>GenAI models are trained on data, so it is important to use a variety of data to reduce bias and improve accuracy. This includes using data from different sources, different demographics, and different viewpoints.</p>"},{"location":"lab1/best-practices/#2-monitor-the-output-of-genai-models-carefully","title":"2. Monitor the output of GenAI models carefully.","text":"<p>GenAI models can produce incorrect or biased results, so it is important to monitor their output carefully. This includes manually reviewing the output for errors and bias, and using automated tools to identify and remove harmful content.</p>"},{"location":"lab1/best-practices/#3-use-genai-models-in-a-responsible-and-ethical-way","title":"3. Use GenAI models in a responsible and ethical way.","text":"<p>GenAI models can be used to generate harmful content, such as misinformation and disinformation. It is important to use GenAI models in a responsible and ethical way, and to avoid using them to generate content that could harm others.</p>"},{"location":"lab1/best-practices/#4-be-transparent-about-the-use-of-genai","title":"4. Be transparent about the use of GenAI.","text":"<p>When using GenAI to generate content, it is important to be transparent about the fact that GenAI was used. This allows people to be aware of the limitations of the content and to make informed decisions about whether or not to trust it.</p>"},{"location":"lab1/best-practices/#5-establish-ethical-guidelines-for-the-use-of-genai","title":"5. Establish ethical guidelines for the use of GenAI.","text":"<p>Organizations should establish ethical guidelines for the use of GenAI. These guidelines should cover topics such as the use of GenAI to generate harmful content, the transparency of GenAI use, and the privacy and security of GenAI data.</p> <p>Here are some additional best practices for using GenAI:</p> <ul> <li>Use GenAI models that have been developed by reputable organizations.</li> <li>Keep your GenAI models up to date with the latest training data and security patches.</li> <li>Back up your GenAI models regularly.</li> <li>Have a plan for how to handle errors and bias in the output of your GenAI models.</li> <li>Be aware of the potential legal and regulatory implications of using GenAI.</li> </ul> <p>By following these best practices, you can help to ensure that GenAI is used in a responsible and ethical way.</p> <p>Here are some specific examples of how to apply these best practices in real-world situations:</p> <ul> <li>A company that is using GenAI to generate marketing materials should use a variety of training data that includes data from different demographics and viewpoints. They should also monitor the output of their GenAI models carefully to identify and remove any biased or harmful content.</li> <li>A news organization that is using GenAI to generate news articles should be transparent about the fact that GenAI was used. They should also have a plan for how to handle errors and bias in the output of their GenAI models.</li> <li>A government agency that is using GenAI to develop new policies should establish ethical guidelines for the use of GenAI. These guidelines should cover topics such as the use of GenAI to generate harmful content, the transparency of GenAI use, and the privacy and security of GenAI data.</li> </ul> <p>GenAI is a powerful new technology with the potential to revolutionize many industries. By following the best practices outlined above, you can help to ensure that GenAI is used in a responsible and ethical way.</p>"},{"location":"lab1/challenges/","title":"Current Challenges","text":"<p>Generative AI (GenAI) has the potential to revolutionize many industries, but there are still some challenges that need to be addressed before it can be widely adopted. Here are the top five GenAI challenges and how to overcome them:</p>"},{"location":"lab1/challenges/#1-limited-dataset-availability","title":"1. Limited dataset availability","text":"<p>GenAI models are trained on massive datasets of text and code. However, it can be difficult to find high-quality datasets that are relevant to the specific application. Additionally, some datasets may be biased or contain false information.</p>"},{"location":"lab1/challenges/#how-to-overcome-it","title":"How to overcome it:","text":"<p>Use multiple datasets: To reduce bias and improve accuracy, GenAI models should be trained on multiple datasets from different sources. Clean the data: Before training a GenAI model, the data should be cleaned to remove any false, biased, or contradicting information. This can be done manually or using automated tools. Use synthetic data: If high-quality datasets are not available, synthetic data can be generated. Synthetic data is artificially created data that is similar to real-world data.</p>"},{"location":"lab1/challenges/#2-explainability-and-interpretability","title":"2. Explainability and interpretability","text":"<p>GenAI models can be complex and difficult to understand. This makes it difficult to explain why a model makes a particular decision or generates a particular output.</p>"},{"location":"lab1/challenges/#how-to-overcome-it_1","title":"How to overcome it:","text":"<p>Develop interpretable models: Researchers are developing new GenAI models that are more interpretable. These models allow users to see how the model's inputs are mapped to its outputs. Use post-hoc explanation techniques: Post-hoc explanation techniques can be used to explain the predictions of existing GenAI models. These techniques work by analyzing the model's internal state and identifying the features that were most important in making the prediction.</p>"},{"location":"lab1/challenges/#3-computing-resources","title":"3. Computing resources","text":"<p>Training GenAI models requires a lot of computing resources. This can be a barrier to entry for small businesses and organizations.</p>"},{"location":"lab1/challenges/#how-to-overcome-it_2","title":"How to overcome it:","text":"<p>Use cloud computing: Cloud computing platforms provide access to powerful computing resources on demand. This can make it more affordable and accessible to train GenAI models. Use distributed training: Distributed training techniques can be used to train GenAI models on multiple machines. This can speed up the training process and reduce the cost.</p>"},{"location":"lab1/challenges/#4-legal-and-regulatory-frameworks","title":"4. Legal and regulatory frameworks","text":"<p>There are no clear legal and regulatory frameworks in place for the use of GenAI. This can create uncertainty and risk for businesses and organizations.</p>"},{"location":"lab1/challenges/#how-to-overcome-it_3","title":"How to overcome it:","text":"<p>Develop industry standards: Industry associations and organizations can work together to develop standards for the use of GenAI. These standards can help to ensure that GenAI is used in a responsible and ethical manner. Advocate for clear regulations: Businesses and organizations can advocate for clear and fair regulations for the use of GenAI. This will help to create a more predictable and stable environment for GenAI innovation.</p>"},{"location":"lab1/challenges/#5-ethical-considerations","title":"5. Ethical considerations","text":"<p>GenAI raises a number of ethical concerns, such as the potential for bias, misuse, and job displacement. It is important to address these concerns before GenAI is widely adopted.</p>"},{"location":"lab1/challenges/#how-to-overcome-it_4","title":"How to overcome it:","text":"<p>Develop ethical guidelines: Businesses and organizations should develop ethical guidelines for the use of GenAI. These guidelines should address how to avoid bias, misuse, and other potential harms. Educate the public: It is important to educate the public about the potential benefits and risks of GenAI. This will help to build trust and acceptance for this new technology.</p> <p>GenAI has the potential to revolutionize many industries, but there are still some challenges that need to be addressed before it can be widely adopted. By addressing these challenges, we can ensure that GenAI is used in a responsible and ethical manner to benefit society as a whole.# Challenges</p>"},{"location":"lab1/getting-started/","title":"Getting Started","text":""},{"location":"lab1/overview/","title":"Overview of GenAI","text":""},{"location":"lab1/overview/#what-is-genai","title":"What is GenAI?","text":"<p>Generative AI (GenAI) is a branch of artificial intelligence focused on creating new, original content by learning from vast amounts of data. This technology can generate text, images, audio, and more, mimicking the characteristics of the input data without directly replicating it. GenAI has seen significant advancements through models like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and transformers, which have enabled the production of complex and realistic outputs, pushing the boundaries of content creation and automation.</p> <p>In real-world applications, GenAI is making strides across various industries, from entertainment and pharmaceuticals to manufacturing and finance. For instance, it's used in the entertainment industry for generating conceptual art and music for games and movies, while pharmaceutical companies leverage it for designing proteins for new medicines. Manufacturing sectors utilize GenAI for optimizing design processes in 3D printing and additive manufacturing, demonstrating GenAI's versatility in enhancing creativity, efficiency, and personalization across different fields.</p> <p>The advent of Large Language Models (LLMs) has been a game-changer in the field of GenAI, particularly in text generation. Models like GPT (Generative Pre-trained Transformer) series by OpenAI have showcased the ability to generate human-like text, offering applications in customer support, content creation, and more. These LLMs are trained on diverse internet text, enabling them to produce coherent and contextually relevant text outputs based on prompts. The integration of LLMs into GenAI applications exemplifies the technology's potential in transforming communication, content generation, and interactive experiences.</p> <p>These developments underscore the expansive potential of GenAI, from revolutionizing content creation to accelerating drug discovery and beyond, showcasing a future where AI-driven innovation is integral across sectors.</p>"},{"location":"lab1/overview/#what-is-genai-stack","title":"What is GenAI Stack?","text":"<p>The GenAI Stack is a set of Docker containers that are orchestrated by Docker Compose which includes a management tool for local LLMs (Ollama), a database for grounding (Neo4j), and GenAI apps based on LangChain. The containers provide a dev environment of a pre-built, support agent app with data import and response generation use-cases. You can experiment with importing different information in the knowledge graph and examine how the variety in underlying grounding information affects the generated responses by the LLM in the user interface.</p> <p>The GenAI Stack consists of:</p> <ul> <li>Application containers (the application logic in Python built with LangChain for the orchestration and Streamlit for the UI).</li> <li>Database container with vector index and graph search (Neo4j).</li> <li>LLM container Ollama (if you\u2019re on Linux). If you\u2019re on MacOS, install Ollama outside of Docker.</li> </ul> <p>These containers are tied together with Docker compose. Docker compose has a watch mode setup that rebuilds relevant containers any time you make a change to the application code, allowing for fast feedback loops and a good developer experience.</p>"},{"location":"lab1/solution/","title":"What problem does it solve","text":"<p>Generative AI (GenAI) is a type of artificial intelligence that can create new content, such as text, images, and music. It is still under development, but it has the potential to revolutionize many industries and solve a wide range of problems.</p> <p>Here are some of the problems that GenAI can solve:</p>"},{"location":"lab1/solution/#content-creation","title":"Content creation","text":"<p>GenAI can be used to create high-quality content at scale. This can be useful for businesses that need to produce a lot of content, such as marketing materials, blog posts, and social media posts. GenAI can also be used to create personalized content for each user, such as product recommendations and news feeds.</p>"},{"location":"lab1/solution/#data-augmentation","title":"Data augmentation","text":"<p>GenAI can be used to generate synthetic data, which can be used to train machine learning models and improve their performance. This is especially useful for applications where it is difficult or expensive to collect real-world data. Creative tasks: GenAI can be used to automate creative tasks, such as writing poetry, generating music, and designing images. This can free up human creativity for more complex and strategic tasks.</p>"},{"location":"lab1/solution/#problem-solving","title":"Problem solving","text":"<p>GenAI can be used to solve complex problems by generating creative solutions. For example, GenAI can be used to design new drugs, develop new products, and optimize business processes.</p> <p>GenAI is still in its early stages of development, but it has the potential to solve a wide range of problems and revolutionize many industries. Here are a few specific examples of how GenAI is being used today:</p>"},{"location":"lab1/solution/#marketing","title":"Marketing","text":"<p>GenAI is being used to create personalized marketing campaigns and generate targeted content for each user.</p>"},{"location":"lab1/solution/#media","title":"Media","text":"<p>GenAI is being used to create personalized news feeds and generate realistic synthetic media, such as videos and images.</p>"},{"location":"lab1/solution/#healthcare","title":"Healthcare","text":"<p>GenAI is being used to design new drugs, develop personalized treatment plans, and diagnose diseases.</p>"},{"location":"lab1/solution/#finance","title":"Finance","text":"<p>GenAI is being used to detect fraud, develop trading strategies, and optimize risk management.</p>"},{"location":"lab1/solution/#science","title":"Science","text":"<p>GenAI is being used to accelerate scientific research by generating new hypotheses and designing experiments.</p>"},{"location":"lab2/compose-profile/","title":"Compose Profile","text":""},{"location":"lab2/compose-profile/#clone-the-repository","title":"Clone the repository","text":"<pre><code> git clone https://github.com/dockersamples/docker-init-demos/\n</code></pre>"},{"location":"lab2/compose-profile/#change-directory-to-python-project","title":"Change directory to Python project","text":"<pre><code> cd docker-init-demos/python/compose-watch/sample\n</code></pre>"},{"location":"lab2/compose-profile/#start-the-compose-services","title":"Start the Compose services","text":"<p>Ensuring that you have stopped old compose services so as not to conflict between the listening ports</p> <pre><code> docker compose --profile dev --file compose-profile.yaml up -d\n</code></pre> <p>The command will start just 2 services - Python and database.</p> <p>Change dev to prod, and see if it starts all the listed services</p>"},{"location":"lab2/compose-watch/","title":"Overview of Compose Watch","text":""},{"location":"lab2/compose-watch/#compose-watch","title":"Compose Watch","text":"<p>Use watch to automatically update and preview your running Compose services as you edit and save your code. Compose Watch is available in Docker Compose version 2.22 and later.</p>"},{"location":"lab2/compose-watch/#clone-the-repository","title":"Clone the repository","text":"<pre><code> git clone https://github.com/dockersamples/docker-init-demos\n</code></pre>"},{"location":"lab2/compose-watch/#change-to-the-python-directory","title":"Change to the python directory","text":"<pre><code> cd docker-init-demos/python/compose-watch\n</code></pre>"},{"location":"lab2/compose-watch/#copy-the-necessary-files-from-samples-directory","title":"Copy the necessary files from samples directory","text":"<pre><code> cp -rf samples/Dockerfile .\n cp -rf samples/compose.yaml .\n</code></pre>"},{"location":"lab2/compose-watch/#start-the-compose-watch","title":"Start the Compose Watch","text":"<pre><code> docker compose watch\n</code></pre> <p>Keep the terminal open. Open a new terminal and make some changes in the requirements.txt:</p> <pre><code> flask\n gunicorn\n</code></pre> <p>Examine the compose watch terminal to see how it rebuilds the application automatically.</p> <p>You will see that compose watch monitors the file system change and rebuilds it automatically.</p>"},{"location":"lab2/docker-developer-workflow/","title":"Docker Developer Workflow","text":""},{"location":"lab2/getting-started/","title":"Getting Started with Docker Init","text":"<p>Example used to demonstrate <code>docker init</code> CLI for a simple Hello World Python Program</p>"},{"location":"lab2/getting-started/#clone-the-repository","title":"Clone the repository","text":"<pre><code> git clone https://github.com/dockersamples/docker-init-demos\n cd docker-init-demos/python\n</code></pre>"},{"location":"lab2/getting-started/#run-the-application","title":"Run the application","text":"<p>You can simply use <code>python3 app.py</code> command.</p> <p>This code defines a handler that responds to GET requests with the specified text and starts an HTTP server listening on port 8080. When you run the script, you can access the server at http://localhost:8080 and see the same message as the Python program.</p> <p>Those commands will start a http server listening on port <code>8080</code>  and if your request <code>http://localhost:8080</code> you'll see the following output: </p> <pre><code>curl http://localhost:8080\n\n          ##         .\n    ## ## ##        ==\n ## ## ## ## ##    ===\n/\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===\n{                       /  ===-\n\\______ O           __/\n \\    \\         __/\n  \\____\\_______/\n\n\nHello from Docker!\n</code></pre>"},{"location":"lab2/getting-started/#using-docker-init","title":"Using Docker init","text":""},{"location":"lab2/getting-started/#run-the-following-command","title":"Run the following command:","text":"<pre><code> docker init\n</code></pre> <p>This utility will walk you through creating the following files with sensible defaults for your project:   - .dockerignore   - Dockerfile   - docker-compose.yaml</p>"},{"location":"lab2/getting-started/#modify-the-dockerfile","title":"Modify the Dockerfile","text":"<pre><code>FROM python:3.8-alpine\nRUN mkdir /app\nADD . /app\nWORKDIR /app\nCMD [\"python3\", \"app.py\"]\n</code></pre>"},{"location":"lab2/getting-started/#modify-the-docker-compose-file","title":"Modify the Docker Compose file","text":"<pre><code>version: '3'\n\nservices:\n  app:\n    build: .\n    ports:\n      - \"8080:8080\"\n    command: python3 app.py\n</code></pre>"},{"location":"lab2/getting-started/#running-the-container-service","title":"Running the container service","text":"<pre><code> docker compose up -d --build\n</code></pre> <p>## Accessing the Python app</p> <pre><code>curl localhost:8080\n\n          ##         .\n    ## ## ##        ==\n ## ## ## ## ##    ===\n/\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===\n{                       /  ===-\n\\______ O           __/\n \\    \\         __/\n  \\____\\_______/\n\n\nHello from Docker!\n</code></pre>"},{"location":"lab2/image-building/","title":"Docker Image Building and Best Practices","text":""},{"location":"lab2/image-building/#introducing-docker-init","title":"Introducing Docker Init","text":"<p>Introduced for the first time in Docker Desktop 4.18, the new docker init CLI generates Docker assets for projects, making it easier to create Docker images and containers. When you run the docker init command in your project directory, it will guide you through the creation of the necessary files for your project with sensible defaults. These files include:</p> <pre><code>.dockerignore\nDockerfile\ndocker-compose.yaml\n</code></pre> <p>The docker init command also allows you to choose the application platform that your project uses and the relative directory of your main package. </p>"},{"location":"lab2/image-building/#whos-this-for","title":"Who\u2019s this for?","text":"<p>This feature is targeted at developers who want to quickly create and manage Docker assets without having to manually configure everything. </p> <p>Note: Currently, the CLI is in beta phase. </p>"},{"location":"lab2/image-building/#benefits-of-docker-init","title":"Benefits of Docker Init","text":"<p>The advantages of using the docker init command include:</p> <ul> <li>Simplified Docker asset creation: The command streamlines the creation of necessary Docker files, reducing the chances of errors and ensuring that best practices are followed.</li> <li>Saves time and effort: With the default settings and guided prompts, users can quickly create Docker assets without the need for extensive knowledge of Docker or its syntax.</li> <li>Better project organization: The generated files provide a standardized and organized structure for the project, making it easier for developers to maintain and update the project over time.</li> <li>Enhanced portability: By using Docker assets, projects become more portable across different environments, making it easier to move the project from development to production.</li> </ul> <p></p>"},{"location":"lab2/inner-loop-developer/","title":"Inner-loop Developer Workflow","text":""},{"location":"lab2/what-is-docker/","title":"What is Docker","text":"<p>Developing apps today requires so much more than writing code. Multiple languages, frameworks, architectures, and discontinuous interfaces between tools for each lifecycle stage creates enormous complexity. Docker simplifies and accelerates your workflow, while giving developers the freedom to innovate with their choice of tools, application stacks, and deployment environments for each project.</p> <p>In 2013, Docker introduced what would become the industry standard for containers. Containers are a standardized unit of software that allows developers to isolate their app from its environment, solving the \u201cit works on my machine\u201d headache. For millions of developers today, Docker is the de facto standard to build and share containerized apps \u2013 from desktop, to the cloud. We are building on our unique connected experience from code to cloud for developers and developer teams.</p> <p>Docker has been around for over 10-years. We started in the container space focused on making it easier for organizations to modernize with container technology. Over the years we have evolved our initial offering to meet the needs of modern developers, focusing on enabling developers to build container based applications locally. </p>"},{"location":"lab2/what-is-docker/#overview-of-docker-desktop","title":"Overview of Docker Desktop","text":"<p>Docker Desktop is the easiest way for developers using containers in production to edit, run, test, and share containerized apps locally on Linux, Windows, and macOS machines. Docker Desktop works out-of-the-box \u2014 meaning it comes with the entire environment needed to develop and run Linux, Windows, and macOS containers locally (on the developer\u2019s laptop). Docker Desktop also handles many of the tedious and complex tasks associated with setting up containers. This allows developers to continue focusing on what they love and do best \u2014 writing code \u2014 instead of spending hours setting up and maintaining Docker locally (i.e., updating the VM, networking, and file sharing mapping between the host and the VM and all the tooling needed to run Docker locally). </p>"},{"location":"lab2/what-is-docker/#docker-is-focused-on-developer-success","title":"Docker is focused on Developer Success","text":"<p>Docker Desktop is a powerful tool designed to simplify and streamline the development and deployment of applications using Docker containers. Let's break down the key points mentioned in the description:</p> <p></p>"},{"location":"lab2/what-is-docker/#speed","title":"Speed","text":"<p>Docker Desktop helps developers maximize their productivity by reducing the time spent on setup and configuration. With Docker, developers can easily package their applications and all their dependencies into containers, allowing for a consistent and reproducible development environment. This eliminates the need for complex manual setups and ensures that everyone on the team can have the same development environment, minimizing the setup overhead.</p>"},{"location":"lab2/what-is-docker/#security","title":"Security","text":"<p>Docker Desktop is designed to provide non-intrusive and actionable security features. It allows developers to identify and address security vulnerabilities during the development process, known as the \"inner loop,\" rather than waiting for later stages like continuous integration (CI) or production. Docker also provides features like container isolation, which enhances security by ensuring that applications run in isolated environments with restricted access to the host system.</p>"},{"location":"lab2/what-is-docker/#choice","title":"Choice","text":"<p>Docker Desktop empowers developers with the freedom to explore and experiment with various technologies. Docker's containerization approach allows developers to use different tools and technologies for different parts of their applications without conflicts or compatibility issues. Developers are not limited to monolithic and \"one-size-fits-all\" development environments; instead, they can choose the best tools for their specific use cases. Overall, Docker Desktop's features cater to developers' needs for a fast and efficient development process, enhanced security practices, and the flexibility to use diverse technologies based on the requirements of their projects. This combination of speed, security, and choice makes Docker Desktop a popular choice among developers looking to containerize and manage their applications effectively.</p>"},{"location":"lab3/cypher/","title":"Intro to Cypher Query Language","text":"<p>Cypher is a declarative graph query language that allows for efficient querying and updating of graph databases, such as those managed by Neo4j. It's designed to be intuitive and to closely resemble English, making it easier to express complex queries and manipulations of graph data. It uses an ASCII-art style syntax consisting of brackets, dashes and arrows.</p> <p></p> <p>You can use Cypher to perform a variety of operations on a graph database, including:</p> <ul> <li>Creating nodes and relationships: You can define new nodes and the relationships between them.</li> <li>Querying data: Cypher allows you to retrieve nodes, relationships, and properties based on specific criteria.</li> <li>Updating data: You can use Cypher to modify existing nodes and relationships, including adding or updating properties.</li> <li>Deleting data: Nodes, relationships, and properties can be removed from the graph.</li> </ul> <p>Cypher queries typically involve specifying patterns that describe the shape of the data you're interested in. These patterns can include nodes, relationships, and the directions of those relationships.</p>"},{"location":"lab3/cypher/#cypher-syntax","title":"Cypher Syntax","text":"<p>Let's dive deeper into Cypher, providing some syntax examples and explanations for common operations you can perform on a graph database using this language.</p>"},{"location":"lab3/cypher/#creating-nodes-and-relationships","title":"Creating Nodes and Relationships","text":"<p>To create a node, you use the <code>CREATE</code> statement. Nodes can have labels (which can be thought of as types or categories) and properties (key-value pairs).</p> <pre><code>CREATE (n:Person {name: 'Alice', age: 30})\n</code></pre> <p>This creates a node with the label <code>Person</code> and two properties: <code>name</code> with the value 'Alice' and <code>age</code> with the value 30.</p> <p>To create a relationship between two nodes, you also use the <code>CREATE</code> statement. Relationships have types and can also have properties.</p> <pre><code>CREATE (n:Person {name: 'Alice'})-[:FRIEND_OF {since: 2021}]-&gt;(m:Person {name: 'Bob'})\n</code></pre> <p>This creates two <code>Person</code> nodes and a <code>FRIEND_OF</code> relationship from Alice to Bob, with a property since indicating the year they became friends.</p>"},{"location":"lab3/cypher/#querying-data","title":"Querying Data","text":"<p>To retrieve data, you use the <code>MATCH</code> statement, which allows you to specify patterns to find in the graph. You can also use <code>WHERE</code> to filter results, and <code>RETURN</code> to specify what to return.</p> <pre><code>MATCH (n:Person {name: 'Alice'})\nRETURN n\n</code></pre> <p>This query looks for a <code>Person</code> node with the name 'Alice' and returns it.</p> <p>You can also query relationships:</p> <pre><code>MATCH (n:Person)-[r:FRIEND_OF]-&gt;(m:Person)\nWHERE n.name = 'Alice'\nRETURN n, r, m\n</code></pre> <p>This finds <code>Person</code> nodes that are friends of Alice and returns the nodes and the relationships.</p>"},{"location":"lab3/cypher/#updating-data","title":"Updating Data","text":"<p>To update data, you can use <code>SET</code> to add or change properties of nodes and relationships.</p> <pre><code>MATCH (n:Person {name: 'Alice'})\nSET n.age = 31\nRETURN n\n</code></pre> <p>This updates Alice's age to 31.</p>"},{"location":"lab3/cypher/#deleting-data","title":"Deleting Data","text":"<p>To delete nodes and relationships, you use the <code>DELETE</code> statement.</p> <pre><code>MATCH (n:Person {name: 'Alice'})\nDELETE n\n</code></pre> <p>This deletes the <code>Person</code> node with the name 'Alice'. Note that you must delete or detach all relationships of a node before you can delete the node itself.</p>"},{"location":"lab3/cypher/#aggregation-and-ordering","title":"Aggregation and Ordering","text":"<p>Cypher supports aggregation functions similar to SQL, as well as ordering results.</p> <pre><code>MATCH (n:Person)\nRETURN n.age, COUNT(n) AS num_people\nORDER BY n.age\n</code></pre> <p>This query returns the ages of all <code>Person</code> nodes and the count of people for each age, ordered by age.</p>"},{"location":"lab3/cypher/#combining-operations","title":"Combining Operations","text":"<p>You can combine these operations to perform complex queries and updates. For example, you might want to find all friends of Alice, increment their ages, and return the updated nodes:</p> <pre><code>MATCH (n:Person {name: 'Alice'})-[:FRIEND_OF]-&gt;(friend:Person)\nSET friend.age = friend.age + 1\nRETURN friend\n</code></pre> <p>This finds all of Alice's friends, increments their ages by 1, and returns the updated friend nodes.</p> <p>Cypher's syntax and operations are designed to be intuitive for dealing with graph structures, making it easier to express complex queries involving nodes, relationships, and properties.</p>"},{"location":"lab3/intro/","title":"Introduction to Neo4j and Knowledge Graphs","text":""},{"location":"lab3/intro/#introduction-to-neo4j","title":"Introduction to Neo4j","text":"<p>Neo4j is a highly popular graph database management system, designed for storing and querying interconnected data. Unlike traditional relational databases that store data in tables, Neo4j is based on graph theory, using nodes, relationships, and properties to represent and store data. The key components of Neo4j include:</p> <ul> <li>Nodes: Entities or objects such as people, businesses, accounts, or any item you want to track.</li> <li>Relationships: Connections between nodes, which can be directed and named and may carry properties. These relationships provide context and add richness to the data model.</li> <li>Properties: Key-value pairs that store data associated with nodes and relationships, allowing for the storage of detailed information about the entities in the graph.</li> </ul> <p></p> <p>Neo4j is particularly well-suited for applications that require complex queries and analysis of connected data, such as social networks, recommendation engines, fraud detection, and more. Its Cypher query language is specially designed for querying graph data in an expressive and efficient manner.</p> <p>You can get started with Neo4j with AuraDB and for more learning content check out Free, Self-Paced, Hands-on Online Training Courses at Neo4j GraphAcademy.</p>"},{"location":"lab3/intro/#introduction-to-knowledge-graphs","title":"Introduction to Knowledge Graphs","text":"<p>Knowledge Graphs are a form of graph-based data representation that organize and integrate information in a way that makes it possible for computers to understand and interpret. They consist of nodes representing entities (such as people, places, concepts, and objects) and edges representing the relationships between these entities. Knowledge graphs are used to store interconnected descriptions of entities with free-form semantics, allowing for a dynamic and interconnected representation of knowledge.</p> <p></p> <p>The power of knowledge graphs lies in their flexibility and the richness of the semantics they can capture, making them an essential tool for many applications, including:</p> <ul> <li>Semantic Search: Enhancing search capabilities by understanding the context and relationships between terms.</li> <li>Recommendation Systems: Offering personalized recommendations based on the interconnected nature of user interests and behavior.</li> <li>Natural Language Processing (NLP): Improving the understanding of human language by mapping out relationships and entities in text.</li> <li>Data Integration: Unifying data from multiple sources and formats, providing a holistic view of information.</li> </ul> <p>Knowledge graphs can be implemented using various storage technologies, including graph databases like Neo4j, which provides a robust platform for managing complex, connected data structures.</p>"},{"location":"lab3/intro/#relationship-between-neo4j-and-knowledge-graphs","title":"Relationship between Neo4j and Knowledge Graphs","text":"<p>Knowledge graphs are a specific implementation of a Graph Database, where information is captured and integrated from many different sources, representing the inherent knowledge of a particular domain.</p> <p>They provide a structured way to represent entities, their attributes, and their relationships, allowing for a comprehensive and interconnected understanding of the information within that domain.</p> <p>Knowledge graphs break down sources of information and integrate them, allowing you to see the relationships between the data.</p> <p></p> <p>You can tailor knowledge graphs for semantic search, data retrieval, and reasoning.</p> <p>You may not be familiar with the term knowledge graph, but you have probably used one. Search engines typically use knowledge graphs to provide information about people, places, and things.</p> <p>Neo4j can be used as the underlying database technology for building knowledge graphs. Its graph-based model aligns naturally with the structure of knowledge graphs, enabling efficient storage, querying, and management of interconnected data. By leveraging Neo4j, developers can create powerful applications that harness the full potential of knowledge graphs, from complex data analysis to AI-driven insights.</p> <p>In summary, Neo4j and Knowledge Graphs represent a powerful paradigm for managing and analyzing interconnected data. They offer a flexible and semantically rich way to model relationships and insights that are difficult to capture with traditional database systems, opening up new possibilities for understanding and leveraging data in various domains.</p>"},{"location":"lab4/best-practices/","title":"Best Practices","text":"<p>Working with frameworks like LangChain, which facilitate the integration of Large Language Models (LLMs) into applications, involves a set of best practices aimed at maximizing efficiency, maintaining code quality, and ensuring the ethical use of AI. Here are some best practices to consider when working with LangChain:</p>"},{"location":"lab4/best-practices/#understand-the-fundamentals","title":"Understand the Fundamentals","text":""},{"location":"lab4/best-practices/#1-grasp-llm-capabilities-and-limitations","title":"1. Grasp LLM Capabilities and Limitations:","text":"<ul> <li>Have a solid understanding of what LLMs can and cannot do. Recognize their strengths in language understanding and generation, and their limitations, such as occasional inaccuracy or bias.</li> </ul>"},{"location":"lab4/best-practices/#code-and-architecture-design","title":"Code and Architecture Design","text":""},{"location":"lab4/best-practices/#1-modular-design","title":"1. Modular Design:","text":"<ul> <li>Design your application in a modular way, separating the concerns of language model interaction, business logic, and data handling for better maintainability and scalability.</li> </ul>"},{"location":"lab4/best-practices/#2-use-version-control","title":"2. Use Version Control:","text":"<ul> <li>Always use version control systems like Git to manage changes in your LangChain projects, facilitating collaboration and rollback if necessary.</li> </ul>"},{"location":"lab4/best-practices/#3-handle-secrets-securely","title":"3. Handle Secrets Securely:","text":"<ul> <li>Securely manage API keys and other sensitive information using environment variables or secure vaults, never hard-code them into your application.</li> </ul>"},{"location":"lab4/best-practices/#development-and-testing","title":"Development and Testing","text":""},{"location":"lab4/best-practices/#1-test-thoroughly","title":"1. Test Thoroughly:","text":"<ul> <li>Implement unit and integration tests to ensure that your LangChain integration works as expected and to catch any issues early on.</li> </ul>"},{"location":"lab4/best-practices/#2-monitor-performance","title":"2. Monitor Performance:","text":"<ul> <li>Monitor the performance of the LLM within your application, especially if you\u2019re using paid APIs, to manage costs and efficiency.</li> </ul>"},{"location":"lab4/best-practices/#3-error-handling","title":"3. Error Handling:","text":"<ul> <li>Develop robust error handling and logging to deal with unexpected responses from the LLM or issues in the LangChain framework.</li> </ul>"},{"location":"lab4/best-practices/#ai-ethics-and-compliance","title":"AI Ethics and Compliance","text":""},{"location":"lab4/best-practices/#1-consider-ethical-implications","title":"1. Consider Ethical Implications:","text":"<ul> <li>Be aware of the ethical implications of using LLMs, such as potential biases in model responses, and strive to mitigate them.</li> </ul>"},{"location":"lab4/best-practices/#2-data-privacy","title":"2. Data Privacy:","text":"<ul> <li>Ensure that you are compliant with data privacy regulations (like GDPR or CCPA) when processing user data through LLMs.</li> </ul>"},{"location":"lab4/best-practices/#3-user-consent","title":"3. User Consent:","text":"<ul> <li>Obtain and manage user consent when necessary, especially when user data is processed or stored.</li> </ul>"},{"location":"lab4/best-practices/#performance-and-scalability","title":"Performance and Scalability","text":""},{"location":"lab4/best-practices/#1-cache-responses","title":"1. Cache Responses:","text":"<ul> <li>Cache common responses from the LLM to improve performance and reduce costs associated with API calls.</li> </ul>"},{"location":"lab4/best-practices/#2-use-asynchronous-calls","title":"2. Use Asynchronous Calls:","text":"<ul> <li>Make asynchronous calls to LLMs when possible to improve the responsiveness of your application.</li> </ul>"},{"location":"lab4/best-practices/#deployment-and-maintenance","title":"Deployment and Maintenance","text":""},{"location":"lab4/best-practices/#1-continuous-integrationcontinuous-deployment-cicd","title":"1. Continuous Integration/Continuous Deployment (CI/CD):","text":"<ul> <li>Employ CI/CD practices for automated testing and deployment of your LangChain applications.</li> </ul>"},{"location":"lab4/best-practices/#2-stay-updated","title":"2. Stay Updated:","text":"<ul> <li>Keep the LangChain framework and its dependencies up to date to benefit from the latest features and security patches.</li> </ul>"},{"location":"lab4/best-practices/#3-monitor-usage-and-quotas","title":"3. Monitor Usage and Quotas:","text":"<ul> <li>Keep an eye on your API usage to avoid unexpected charges and to ensure that you stay within your quota.</li> </ul> <p>By adhering to these best practices, you can create robust, efficient, and ethical applications using LangChain and Large Language Models.</p>"},{"location":"lab4/getting-started/","title":"Getting Started","text":"<p>Getting started with LangChain involves a few key steps that allow developers to begin integrating Large Language Models (LLMs) into their applications. Below is a guide to help you get started with LangChain:</p>"},{"location":"lab4/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before diving into LangChain, ensure you have the following:</p> <ul> <li>A working knowledge of Python, as LangChain is heavily Python-based.</li> <li>Familiarity with virtual environments in Python for package management (e.g., <code>venv</code> or <code>conda</code>).</li> <li>An understanding of LLMs and how they operate, at least at a high level.</li> </ul>"},{"location":"lab4/getting-started/#installation","title":"Installation","text":""},{"location":"lab4/getting-started/#1-environment-setup","title":"1. Environment Setup:","text":"<ul> <li>Create a new virtual environment to keep your dependencies organized and project-specific.</li> </ul> <pre><code>python -m venv langchain-env\nsource langchain-env/bin/activate  # On Windows, use `langchain-env\\Scripts\\activate`\n</code></pre>"},{"location":"lab4/getting-started/#2-install-langchain","title":"2. Install LangChain:","text":"<ul> <li>With your environment activated, use <code>pip</code> to install LangChain.</li> </ul> <pre><code>pip install langchain\n</code></pre>"},{"location":"lab4/getting-started/#configuration","title":"Configuration","text":""},{"location":"lab4/getting-started/#1-api-keys","title":"1. API Keys:","text":"<ul> <li>If you plan to use third-party LLMs like OpenAI's GPT, make sure to sign up for access and retrieve your API keys.</li> </ul>"},{"location":"lab4/getting-started/#2-langchain-settings","title":"2. LangChain Settings:","text":"<ul> <li>Set up your LangChain configuration file (langchain.config) with the necessary API keys and default settings.</li> </ul>"},{"location":"lab4/getting-started/#exploring-langchain","title":"Exploring LangChain","text":""},{"location":"lab4/getting-started/#1-documentation","title":"1. Documentation:","text":"<ul> <li>Familiarize yourself with the LangChain documentation. This will give you an overview of the capabilities and modules available.</li> </ul>"},{"location":"lab4/getting-started/#2-tutorials-and-examples","title":"2. Tutorials and Examples:","text":"<ul> <li>Walk through any tutorials provided in the documentation or GitHub repository. These examples can give you a hands-on understanding of how to use LangChain.</li> </ul>"},{"location":"lab4/getting-started/#building-with-langchain","title":"Building with LangChain","text":""},{"location":"lab4/getting-started/#1-start-coding","title":"1. Start Coding:","text":"<ul> <li>Begin by writing simple scripts to interact with LLMs using LangChain. Experiment with sending prompts and processing responses.</li> </ul>"},{"location":"lab4/getting-started/#2-explore-modules","title":"2. Explore Modules:","text":"<ul> <li>Use LangChain modules to handle specific tasks like document loaders, utilities, and agents. These modules can simplify complex tasks.</li> </ul>"},{"location":"lab4/getting-started/#3-develop-applications","title":"3. Develop Applications:","text":"<ul> <li>Start integrating LangChain into your applications. You can create chatbots, automate document analysis, or develop other innovative solutions.</li> </ul>"},{"location":"lab4/intro/","title":"Introduction to Langchain","text":"<p>In an era where artificial intelligence (AI) is reshaping industries, Large Language Models (LLMs) stand at the forefront, driving innovation and creativity. However, harnessing their full potential remains a challenge for developers and businesses alike. Enter LangChain, a cutting-edge framework designed to bridge this gap. LangChain is the key to unlocking the vast capabilities of LLMs, making it easier than ever to integrate advanced AI functionalities into applications.</p>"},{"location":"lab4/intro/#what-is-langchain","title":"What is LangChain?","text":"<p>LangChain is an open-source framework that democratizes access to the capabilities of Large Language Models. It is designed to streamline the development of applications ranging from document analysis and summarization to sophisticated chatbots and intricate code analysis. LangChain acts as a conduit between the complex world of LLMs and practical, real-world applications, providing developers with the tools and APIs necessary to integrate AI functionalities seamlessly. Whether you're working with Python or Javascript, LangChain offers the flexibility and resources needed to innovate and excel.</p>"},{"location":"lab4/intro/#the-significance-of-langchain","title":"The Significance of LangChain","text":"<p>Integrating LLMs into applications can be daunting due to their complexity and the technical expertise required. LangChain emerges as a solution to this challenge, simplifying the development process and making it accessible to a broader range of developers. By providing orchestration tools and APIs, LangChain allows developers to focus on the creative aspects of application development, reducing the technical hurdles associated with LLM integration. This democratization of AI tools paves the way for more innovative, efficient, and intelligent applications, transforming how we interact with technology.</p> <p></p> <p>Source: https://www.kdnuggets.com/2023/04/langchain-101-build-gptpowered-applications.html</p>"},{"location":"lab4/intro/#how-does-it-work","title":"How does it work?","text":"<p>LangChain applications bridge users and LLMs, communicating back and forth with the LLM through Chains.</p> <p>The key components of a Langchain application are:</p> <ul> <li> <p>Model Interaction (Model I/O): Components that manage the interaction with the language model, overseeing tasks like feeding inputs and extracting outputs.</p> </li> <li> <p>Data Connection and Retrieval: Retrieval components can access, transform, and store data, allowing for efficient queries and retrieval.</p> </li> <li> <p>Chains: Chains are reusable components that determine the best way to fulfill an instruction based on a prompt.</p> </li> <li> <p>Agents: Agents orchestrate commands directed at LLMs and other tools, enabling them to perform specific tasks or solve designated problems.</p> </li> <li> <p>Memory: Allow applications to retain context, for example, remembering the previous messages in a conversation.</p> </li> </ul>"},{"location":"lab4/intro/#features-and-capabilities","title":"Features and Capabilities","text":"<p>LangChain is packed with features that cater to the diverse needs of developers. Its support for both Python and Javascript libraries means that developers can work in their preferred programming environment. LangChain's flexibility extends to a wide range of applications, from automating mundane tasks with chatbots to providing insightful analyses of large datasets. Its intuitive API and comprehensive documentation ensure that even those new to LLMs can start building sophisticated applications quickly. LangChain is not just a tool but a partner in innovation, offering scalability, ease of use, and a community of support.</p>"},{"location":"lab4/intro/#practical-applications-of-langchain","title":"Practical Applications of LangChain","text":"<p>The potential applications for LangChain are as vast as the imagination. In customer service, chatbots powered by LangChain can handle inquiries with unprecedented accuracy and nuance, improving user experience. For businesses drowning in data, LangChain can automate document analysis, extracting key insights and summarizing information efficiently. Developers can leverage LangChain for code analysis, enhancing productivity and code quality. These examples barely scratch the surface of what's possible, illustrating LangChain's role as a catalyst for AI-driven solutions across industries.</p>"},{"location":"lab4/llms-intro/","title":"Introduction to LLMs","text":"<p>Large Language Models, referred to as LLMs, learn the underlying structure and distribution of the data and can then generate new samples that resemble the original data.</p> <p>LLMs are trained on vast amounts of text data to understand and generate human-like text. LLMs can answer questions, create content, and assist with various linguistic tasks by leveraging patterns learned from the data.</p> <p>Generative AI is a class of algorithms and models that can generate new content, such as images, text, or even music. New content is generated based on user prompting, existing patterns, and examples from existing data.</p>"},{"location":"lab4/llms-intro/#instructing-an-llm","title":"Instructing an LLM","text":"<p>The response generated by an LLM is a probabilistic continuation of the instructions it receives. The LLM provides the most likely response based on the patterns it has learned from its training data.</p> <p>In simple terms, if presented with the prompt \"Continue this sequence - A B C\", an LLM could respond \"D E F\".</p> <p>To get an LLM to perform a task, you provide a prompt, a piece of text that should specify your requirements and provide clear instructions on how to respond.</p> <p></p> <p>Precision in the task description, potentially combined with examples or context, ensures that the model understands the intent and produces relevant and accurate outputs.</p> <p>An example prompt may be a simple question.</p> <pre><code>What is the capital of Japan?\n</code></pre> <p>Or, it could be more descriptive. For example:</p> <pre><code>Tell me about the capital of Japan.\nProduce a brief list of talking points exploring its culture and history.\nThe content should be targeted at tourists.\nYour readers may have English as a second language, so use simple terms and avoid colloquialisms.\nAvoid Jargon at all costs.\nReturn the results as a list of JSON strings containing content formatted in Markdown.\n</code></pre> <p>The LLM will interpret these instructions and return a response based on the patterns it has learned from its training data.</p>"},{"location":"lab5/intro-to-ollama/","title":"Introduction to Ollama","text":"<p>The landscape of generative AI is rapidly evolving, with Large Language Models (LLMs) leading the charge. However, running these models typically requires significant computing resources and often involves cloud platforms. Ollama changes the game by offering a solution to run LLMs locally, providing both privacy and performance. This blog post delves into what Ollama is, its features, and how you can get started with it.</p>"},{"location":"lab5/intro-to-ollama/#what-is-ollama","title":"What is Ollama?","text":"<p>Ollama is a platform that allows you to run generative AI LLMs directly on your local machine. With versions available for macOS, Linux, and even Windows through a preview client or the Windows Subsystem for Linux, Ollama brings the power of LLMs to your fingertips without the need for cloud computing services.</p>"},{"location":"lab5/intro-to-ollama/#key-features-of-ollama","title":"Key Features of Ollama:","text":"<ul> <li>Local Processing: All interactions with LLMs happen on your own machine, ensuring privacy and data security.</li> <li>Diverse Model Support: Ollama supports a variety of models, including Llama 2, Code Llama, and others, which can range from 7B to 70B parameters.</li> <li>Customization: The platform allows for customization and creation of your own models, giving you flexibility in your development.</li> <li>CLI and API Access: Ollama provides a simple command-line interface (CLI) as well as a REST API for easy interaction with the models.</li> <li>Docker Support: Recently, Ollama became available as an official Docker image, simplifying the setup process and enabling GPU acceleration, especially for users on macOS and Linux.</li> </ul>"},{"location":"lab5/intro-to-ollama/#getting-started-with-ollama","title":"Getting Started with Ollama:","text":"<ul> <li>Installation: Download Ollama from the official website, ensuring your system meets the necessary requirements, such as the macOS version and available RAM for the model sizes.</li> <li>Running Models: Once installed, you can run models using simple CLI commands, for example, <code>ollama run llama2</code>, to start utilizing the LLMs.</li> <li>Customizing Models: You can customize models by creating a Modelfile and using it to set parameters and system messages, tailoring the model's behavior to your needs.</li> </ul> <p>Ollama stands out as a robust and user-friendly platform that empowers developers and tech enthusiasts to harness the capabilities of LLMs locally. Whether for development, research, or personal projects, Ollama offers a new level of accessibility and control over AI models.</p>"},{"location":"lab6/avoid-hallucination/","title":"Avoiding LLM Hallucination","text":"<p>LLMs are designed to generate human-like text based on the patterns they\u2019ve identified in vast amounts of data.</p> <p>Due to their reliance on patterns and the sheer volume of training information, LLMs sometimes hallucinate or produce outputs that manifest as generating untrue facts, asserting details with unwarranted confidence, or crafting plausible yet nonsensical explanations.</p> <p>These manifestations arise from a mix of overfitting, biases in the training data, and the model\u2019s attempt to generalize from vast amounts of information.</p>"},{"location":"lab6/avoid-hallucination/#common-hallucination-problems","title":"Common Hallucination Problems","text":"<p>Let\u2019s take a closer look at some reasons why this may occur.</p>"},{"location":"lab6/avoid-hallucination/#temperature","title":"Temperature","text":"<p>LLMs have a temperature, corresponding to the amount of randomness the underlying model should use when generating the text.</p> <p>The higher the temperature value, the more random the generated result will become, and the more likely the response will contain false statements.</p> <p>A higher temperature may be appropriate when configuring an LLM to respond with more diverse and creative outputs, but it comes at the expense of consistency and precision.</p> <p>For example, a higher temperature may be suitable for constructing a work of fiction or a novel joke.</p> <p>On the other hand, a lower temperature, even <code>0</code>, is required when a response grounded in facts is essential.</p> <p>A quick fix may be to reduce the temperature. But more likely, the LLM is hallucinating because it hasn\u2019t got the information required.</p>"},{"location":"lab6/avoid-hallucination/#missing-information","title":"Missing Information","text":"<p>The training process for LLMs is intricate and time-intensive, often requiring vast datasets compiled over extended periods. As such, these models might lack the most recent information or might miss out on specific niche topics not well-represented in their training data.</p> <p>For instance, if an LLM\u2019s last update were in September 2022, it would be unaware of world events or advancements in various fields that occurred post that date, leading to potential gaps in its knowledge or responses that seem out of touch with current realities.</p> <p>If the user asks a question on information that is hard to find or outside of the public domain, it will be virtually impossible for an LLM to respond accurately.</p> <p>Luckily, this is where factual information from data sources such as knowledge graphs can help.</p>"},{"location":"lab6/avoid-hallucination/#model-training-and-complexity","title":"Model Training and Complexity","text":"<p>Large Language Models (LLMs) are often considered \"black boxes\" due to the difficulty deciphering their decision-making processes.</p> <p>The complexity of these models, combined with potential training on erroneous or misleading data, means that their outputs can sometimes be unpredictable or inaccurate.</p> <p>For example, an LLM might produce a biased or incorrect answer when asked about a controversial historical event.</p> <p>Furthermore, it would be near impossible to trace back how the model arrived at that conclusion. The LLM would also be unable to provide the sources for its output or explain its reasoning.</p>"},{"location":"lab6/avoid-hallucination/#improving-llm-accuracy","title":"Improving LLM Accuracy","text":"<p>The following methods can be employed to help guide LLMs to produce more consistent and accurate results.</p>"},{"location":"lab6/avoid-hallucination/#prompt-engineering","title":"Prompt Engineering","text":"<p>Prompt engineering is developing specific and deliberate instructions that guide the LLM toward the desired response.</p> <p>By refining how you pose instructions, developers can achieve better results from existing models without retraining.</p> <p>For example, if you require a blog post summary, rather than asking \"<code>What is this blog post about?</code>\", a more appropriate response would be \"<code>Provide a concise, three-sentence summary and three tags for this blog post.</code>\"</p> <p>You could also include \"<code>Return the response as JSON</code>\" and provide an example output to make it easier to parse in the programming language of your choice.</p> <p>Providing additional instructions and context in the question is known as Zero-shot learning.</p>"},{"location":"lab6/avoid-hallucination/#in-context-learning","title":"In-Context Learning","text":"<p>In-context learning provides the model with examples to inform its responses, helping it comprehend the task better.</p> <p>The model can deliver more accurate answers by presenting relevant examples, especially for niche or specialized tasks.</p> <p>Examples could include:</p> <ul> <li> <p>Providing additional context - <code>When asked about \"Bats\", assume the question is about the flying mammal and not a piece of sports equipment.</code></p> </li> <li> <p>Providing examples of the typical input - <code>Questions about capital cities will be formatted as \"What is the capital of {country}?\"</code></p> </li> <li> <p>Providing examples of the desired output - <code>When asked about the weather, return the response in the format \"The weather in {city} is {temperature} degrees Celsius.\"</code></p> </li> </ul> <p>Providing relevant examples for specific tasks is a form of Few-shot learning.</p>"},{"location":"lab6/avoid-hallucination/#fine-tuning","title":"Fine-Tuning","text":"<p>Fine-tuning involves additional language model training on a smaller, task-specific dataset after its primary training phase. This approach allows developers to specialize the model for specific domains or tasks, enhancing its accuracy and relevance.</p> <p>For example, fine-tuning an existing model on your particular businesses would enhance its capability to respond to your customer\u2019s queries.</p> <p>This method is the most complicated, involving technical knowledge, domain expertise, and high computational effort.</p> <p>A more straightforward approach would be to ground the model by providing information with the prompt.</p>"},{"location":"lab6/avoid-hallucination/#grounding","title":"Grounding","text":"<p>Grounding allows a language model to reference external, up-to-date sources or databases to enrich the responses.</p> <p>By integrating real-time data or APIs, developers ensure the model remains current and provides factual information beyond its last training cut-off.</p> <p>For instance, if building a chatbot for a news agency, instead of solely relying on the model\u2019s last training data, grounding could allow the model to pull real-time headlines or articles from a news API. When a user asks, \"What\u2019s the latest news on the Olympics?\", the chatbot, through grounding, can provide a current headline or summary from the most recent articles, ensuring the response is timely and accurate.</p> <p></p>"},{"location":"lab6/genai-stack/","title":"docker-genai-sample","text":"<p>A simple GenAI app for Docker's Docs based on the GenAI Stack PDF Reader application.</p> <p>The generative AI (GenAI) guide teaches you how to containerize an existing GenAI application using Docker. In this guide, you\u2019ll learn how to:</p> <ul> <li>Containerize and run a Python-based GenAI application</li> <li>Set up a local environment to run the complete GenAI stack locally for development</li> <li>Start by containerizing an existing GenAI application.</li> </ul>"},{"location":"lab6/genai-stack/#whats-this-sample-app-all-about","title":"What's this sample app all about?","text":"<p>The sample application used in this guide is a modified version of the PDF Reader application from the GenAI Stack demo applications. The application is a full stack Python application that lets you ask questions about a PDF file.</p> <p>The application uses LangChain for orchestration, Streamlit for the UI, Ollama to run the LLM, and Neo4j to store vectors.</p> <p>Clone the sample application. Open a terminal, change directory to a directory that you want to work in, and run the following command to clone the repository:</p> <pre><code>$ git clone https://github.com/ajeetraina/docker-genai-sample\n</code></pre> <p>You should now have the following files in your <code>docker-genai-sample</code> directory.</p> <pre><code>\u251c\u2500\u2500 docker-genai-sample/\n\u2502 \u251c\u2500\u2500 .gitignore\n\u2502 \u251c\u2500\u2500 app.py\n\u2502 \u251c\u2500\u2500 chains.py\n\u2502 \u251c\u2500\u2500 env.example\n\u2502 \u251c\u2500\u2500 requirements.txt\n\u2502 \u251c\u2500\u2500 util.py\n\u2502 \u251c\u2500\u2500 LICENSE\n\u2502 \u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"lab6/genai-stack/#initialize-docker-assets","title":"Initialize Docker assets","text":"<p>Now that you have an application, you can use <code>docker init</code> to create the necessary Docker assets to containerize your application. Inside the <code>docker-genai-sample</code> directory, run the <code>docker init</code> command. <code>docker init</code> provides some default configuration, but you'll need to answer a few questions about your application. For example, this application uses Streamlit to run. Refer to the following <code>docker init</code> example and use the same answers for your prompts.</p> <pre><code>$ docker init\nWelcome to the Docker Init CLI!\n\nThis utility will walk you through creating the following files with sensible defaults for your project:\n  - .dockerignore\n  - Dockerfile\n  - compose.yaml\n  - README.Docker.md\n\nLet's get started!\n\n? What application platform does your project use? Python\n? What version of Python do you want to use? 3.11.4\n? What port do you want your app to listen on? 8000\n? What is the command to run your app? streamlit run app.py --server.address=0.0.0.0 --server.port=8000\n</code></pre> <p>You should now have the following contents in your <code>docker-genai-sample</code> directory.</p> <pre><code>\u251c\u2500\u2500 docker-genai-sample/\n\u2502 \u251c\u2500\u2500 .dockerignore\n\u2502 \u251c\u2500\u2500 .gitignore\n\u2502 \u251c\u2500\u2500 app.py\n\u2502 \u251c\u2500\u2500 chains.py\n\u2502 \u251c\u2500\u2500 compose.yaml\n\u2502 \u251c\u2500\u2500 env.example\n\u2502 \u251c\u2500\u2500 requirements.txt\n\u2502 \u251c\u2500\u2500 util.py\n\u2502 \u251c\u2500\u2500 Dockerfile\n\u2502 \u251c\u2500\u2500 LICENSE\n\u2502 \u251c\u2500\u2500 README.Docker.md\n\u2502 \u2514\u2500\u2500 README.md\n</code></pre> <p>To learn more about the files that <code>docker init</code> added, see the following:  - Dockerfile  - .dockerignore  - compose.yaml</p>"},{"location":"lab6/genai-stack/#run-the-application","title":"Run the application","text":"<p>Inside the <code>docker-genai-sample</code> directory, run the following command in a terminal.</p> <pre><code>$ docker compose up --build\n</code></pre> <p>Docker builds and runs your application. Depending on your network connection, it may take several minutes to download all the dependencies. You'll see a message like the following in the terminal when the application is running.</p> <pre><code>server-1  |   You can now view your Streamlit app in your browser.\nserver-1  |\nserver-1  |   URL: http://0.0.0.0:8000\nserver-1  |\n</code></pre> <p>Open a browser and view the application at http://localhost:8000. You should see a simple Streamlit application. The application may take a few minutes to download the embedding model. While the download is in progress, Running appears in the top-right corner.</p> <p></p> <p>The application requires a Neo4j database service and an LLM service to function. If you have access to services that you ran outside of Docker, specify the connection information and try it out. If you don't have the services running, continue with this guide to learn how you can run some or all of these services with Docker.</p>"},{"location":"lab6/genai-stack/#starting-neo4j-container","title":"Starting Neo4j container","text":"<pre><code> docker run -d --name database -p 7474:7474 -p 7687:7687 -e NEO4J_AUTH=neo4j/password neo4j:5.11\n</code></pre>"},{"location":"lab6/genai-stack/#starting-ollama-container","title":"Starting Ollama container","text":"<pre><code> docker run -d \\\n  --name ollama \\\n  -p 11434:11434 \\\n  -v ollama_volume:/root/.ollama \\\n  ollama/ollama:latest\n</code></pre>"},{"location":"lab6/genai-stack/#accessing-the-application","title":"Accessing the application","text":"<p>You can populate the form using the following values:</p> <pre><code>Neo4j URI - neo4j://192.168.1.3:7687\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD=password\nOLLAMA_BASE_URL=http://host.docker.internal:11434\nOPENAI Key=&lt;add it here&gt;\n</code></pre>"},{"location":"lab6/genai-stack/#chatting-with-the-pdf","title":"Chatting with the PDF","text":""},{"location":"lab6/llms-hallucination/","title":"LLM Hallucination","text":"<p>While LLMs provide a lot of potential, you should also be cautious.</p> <p>At their core, LLMs are trained to predict the following word(s) in a sequence.</p> <p>The words are based on the patterns and relationships from other text in the training data. The sources for this training data are often the internet, books, and other publicly available text. This data could be of questionable quality and maybe be incorrect. Training happens at a point in time, it may not reflect the current state of the world and would not include any private information.</p> <p>LLMs are fine-tuned to be as helpful as possible, even if that means occasionally generating misleading or baseless content, a phenomenon known as hallucination.</p> <p>For example, when asked to \"Describe the moon.\" and LLM may respond with \"The moon is made of cheese.\". While this is a common saying, it is not true.</p> <p></p> <p>While LLMs can represent the essence of words and phrases, they don\u2019t possess a genuine understanding or ethical judgment of the content.</p> <p>These factors can lead to outputs that might be biased, devoid of context, or lack logical coherence.</p>"},{"location":"lab6/llms-hallucination/#fixing-hallucinations","title":"Fixing Hallucinations","text":"<p>Providing additional contextual data helps to ground the LLM\u2019s responses and make them more accurate.</p> <p>A knowledge graph is a mechanism for providing additional data to an LLM. Data within the knowledge graph can guide the LLM to provide more relevant, accurate, and reliable responses.</p> <p>While the LLM uses its language skills to interpret and respond to the contextual data, it will not disregard the original training data.</p> <p>You can think of the original training data as the base knowledge and linguistic capabilities, while the contextual information guides in specific situations.</p> <p>The combination of both approaches enables the LLM to generate more meaningful responses.</p>"},{"location":"lab6/rag/","title":"Introduction to RAGs","text":"<p>Grounding is the process of providing context to an LLM to improve the accuracy of its responses. For developers and data scientists, grounding usually offers the highest impact for the lowest effort.</p>"},{"location":"lab6/rag/#data-cut-off-dates","title":"Data Cut-Off Dates","text":"<p>Training a Large Language Model has a high computational cost. According to Wikipedia, OpenAI\u2019s GPT-3 model was trained on 175 billion parameters, and the resulting trained model takes 800GB to store.</p> <p>Retraining a model on new data would be expensive and time-consuming. A model may take weeks or months to train.</p> <p>Providing real-time updates on fast-moving breaking news would be out of the question.</p>"},{"location":"lab6/rag/#retrieval-augmented-generation","title":"Retrieval Augmented Generation","text":"<p>A news agency could train a model on static data and supplement the pre-existing knowledge base with real-time data retrieved from up-to-date sources.</p> <p>This approach is known as Retrieval Augmented Generation, or RAG</p> <p>RAG combines the strengths of large-scale language models with external retrieval or search mechanisms, enabling relevant information from vast datasets to be dynamically fed into the model during the generation process, thereby enhancing its ability to provide detailed and contextually accurate responses.</p> <p>In summary, by adding content from additional data sources, you can improve the responses generated by an LLM.</p>"},{"location":"lab6/rag/#benefits-of-rag","title":"Benefits of RAG","text":"<p>The main benefit of RAG is its enhanced accuracy. By dynamically pulling information from external, domain-specific sources, a response grounded by RAG can provide more detailed and contextually accurate answers than a standalone LLM.</p> <p>RAG provides additional benefits of:</p> <ul> <li> <p>Increased transparency, as the sources of the information can be stored and examined.</p> </li> <li> <p>Security, as the data sources can be secured and access controlled.</p> </li> <li> <p>Accuracy and timeliness, as the data sources can be updated in real-time.</p> </li> <li> <p>Access to private or proprietary data</p> </li> </ul> <p>RAG could support the new agency chatbot by:</p> <ol> <li> <p>Accessing real-time new feeds</p> </li> <li> <p>Pulling recent headlines or news articles from a database,</p> </li> <li> <p>Giving this additional context to the LLM</p> </li> </ol> <p>New articles stored in a knowledge graph would be ideal for this use case. A knowledge graph could pass the LLM detail about the relationship between the entities involved and the article\u2019s metadata.</p> <p>For example, when asking about the results of a recent election, the knowledge could provide additional context about the candidates, news stories relating to them, or interesting articles from the same author.</p> <p></p> <p>One of the challenges of RAG is understanding what the user is asking for and surfacing the correct information to pass to the LLM.</p>"},{"location":"lab6/rag/#semantic-search-vs-traditional-keyword-search","title":"Semantic Search vs. Traditional Keyword Search","text":"<p>Semantic search aims to understand search phrases' intent and contextual meaning, rather than focusing on individual keywords.</p> <p>Traditional keyword search often depends on exact-match keywords or proximity-based algorithms that find similar words.</p> <p>For example, if you input \"apple\" in a traditional search, you might predominantly get results about the fruit.</p> <p>However, in a semantic search, the engine tries to gauge the context: Are you searching about the fruit, the tech company, or something else?</p> <p>The results are tailored based on the term and the perceived intent.</p>"},{"location":"lab6/rag/#vectors-and-embeddings","title":"Vectors and embeddings","text":"<p>In natural language processing (NLP) and machine learning, numerical representations (known as vectors) represent words and phrases.</p> <p>Each dimension in a vector can represent a particular semantic aspect of the word or phrase. When multiple dimensions are combined, they can convey the overall meaning of the word or phrase.</p> <p>A vector will not directly encode tangible attributes like color, taste, or shape. Instead, the model will generate a list of numerical values that closely align the word with related words such as health, nutrition, and wellness.</p> <p>When applied in a search context, the vector for \"apple\" can be compared to the vectors for other words or phrases to determine the most relevant results.</p> <p>You can create vectors in various ways, but one of the most common methods is to use a large language model. These vectors are known as embeddings. With advanced models, these embeddings also contain contextual information.</p> <p>For example, the embeddings for the word \"apple\" are <code>0.0077788467, -0.02306925, -0.007360777, -0.027743412, -0.0045747845, 0.01289164, -0.021863015, -0.008587573, 0.01892967, -0.029854324, -0.0027962727, 0.020108491, -0.004530236, 0.009129008,</code> and so on.</p> <p>The vector for a word can change based on its surrounding context. For instance, the word <code>bank</code> will have a different vector in <code>river</code> bank than in <code>savings</code> bank.</p> <p>Semantic search systems can use these contextual embeddings to understand user intent.</p> <p>You can use the <code>distance</code> or <code>angle</code> between vectors to gauge the semantic similarity between words or phrases.</p> <p>Words with similar meanings or contexts will have vectors that are close together, while unrelated words will be farther apart.</p> <p>This principle is employed in semantic search to find contextually relevant results for a user\u2019s query.</p> <p>A semantic search involves the following steps:</p> <ol> <li> <p>The user submits a query.</p> </li> <li> <p>The system creates a vector representation (embedding) of the query.</p> </li> <li> <p>The system compares the query vector to the vectors of the indexed data.</p> </li> <li> <p>The results are scored based on their similarity.</p> </li> <li> <p>The system returns the most relevant results to the user.</p> </li> </ol> <p> Vectors can represent more than just words. They can also represent entire documents, images, audio, or other data types. They are instrumental in the operation of many other machine-learning tasks.</p>"},{"location":"lab6/rag/#vectors-and-neo4j","title":"Vectors and Neo4j","text":"<p>Vectors are the backbone of semantic search. They enable systems to understand and represent the complex, multi-dimensional nature of language, context, and meaning.</p> <p>Since the v5.11 release, Neo4j has a Vector search index, allowing you to query for nodes based on their vector representations.</p>"},{"location":"lab6/using-docker-compose/","title":"Using Docker Compose","text":"<p>Follow the instructions to run a complete GenAI Stack using Docker Compose</p>"},{"location":"lab6/using-docker-compose/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Pre-req</li> <li>Step 1. Install Ollama on Mac OS</li> <li>Step 2. Create OpenAI Secret API Keys</li> <li>Step 3. Sign up for LangChain Beta for API Keys</li> <li>Step 4. Clone the GenAI Repo</li> <li>Step 5. Create .env file</li> <li>Step 6. Bring up Compose Services</li> <li>Step 7. Viewing the services on Docker Dashboard</li> <li>Step 8. Accessing the app</li> <li>Step 9. Accessing the Neo4j </li> <li>Step 10. Accessing GenAI Stack PDF Bot </li> </ul>"},{"location":"lab6/using-docker-compose/#prereq","title":"Prereq","text":"<ul> <li>Install Docker Desktop</li> </ul>"},{"location":"lab6/using-docker-compose/#step-1-install-ollama","title":"Step 1. Install Ollama","text":"<p>Visit this link to download and install Ollama on your preferred operating system.</p> <p></p> <p>Choose your preferrable operating system. </p> <p></p>"},{"location":"lab6/using-docker-compose/#step-2-create-openai-secret-api-keys-optional","title":"Step 2. Create OpenAI Secret API Keys [optional]","text":"<p>Visit this link to create your new OpenAI Secret API Keys.</p>"},{"location":"lab6/using-docker-compose/#step-3-sign-up-for-langchain-beta-for-api-keys-optional-if-you-want-to-enable-langchain-smith-api","title":"Step 3. Sign Up for LangChain Beta for API Keys [optional if you want to enable Langchain Smith API]","text":"<p>Visit this link in order to create Langchain Endpoint and API Keys. You will need the following information</p> <pre><code>LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\nLANGCHAIN_TRACING_V2=true # false\nLANGCHAIN_PROJECT=default\nLANGCHAIN_API_KEY=ls__cbabccXXXXXX\n</code></pre>"},{"location":"lab6/using-docker-compose/#step-4-clone-the-repository","title":"Step 4. Clone the repository","text":"<pre><code> git clone https://github.com/docker/genai-stack\n cd genai-stack\n</code></pre>"},{"location":"lab6/using-docker-compose/#step-5-create-env-file-as-copy-of-envexample-file","title":"Step 5. Create .env file as copy of env.example file","text":"<p>Create a copy of <code>env.example</code> file and rename it as <code>.env</code>. Make sure to update the optional parameters for OpenAI and Langchain if you wish to use them in the project.</p> <pre><code>cat .env \nOPENAI_API_KEY=sk-EsNJzI5uMBCXXXXXXXX\nOLLAMA_BASE_URL=http://host.docker.internal:11434\nNEO4J_URI=neo4j://database:7687\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD=password\nLLM=llama2 #or any Ollama model tag, or gpt-4 or gpt-3.5\nEMBEDDING_MODEL=sentence_transformer #or openai or ollama\n\nLANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\nLANGCHAIN_TRACING_V2=true # false\nLANGCHAIN_PROJECT=default\nLANGCHAIN_API_KEY=ls__cbabccXXXXXX\n</code></pre>"},{"location":"lab6/using-docker-compose/#step-6-bring-up-compose-services","title":"Step 6. Bring up Compose services","text":"<pre><code> docker compose up -d --build\n</code></pre> <pre><code>..\n..\ngenai-stack-bot-1         |   You can now view your Streamlit app in your browser.\ngenai-stack-bot-1         |\ngenai-stack-bot-1         |   URL: http://0.0.0.0:8501\ngenai-stack-bot-1         |\ngenai-stack-pdf_bot-1     |\ngenai-stack-pdf_bot-1     |   You can now view your Streamlit app in your browser.\ngenai-stack-pdf_bot-1     |\ngenai-stack-pdf_bot-1     |   URL: http://0.0.0.0:8503\ngenai-stack-pdf_bot-1     |\ngenai-stack-loader-1      |\ngenai-stack-loader-1      |   You can now view your Streamlit app in your browser.\ngenai-stack-loader-1      |\ngenai-stack-loader-1      |   URL: http://0.0.0.0:8502\ngenai-stack-loader-1      |\n</code></pre>"},{"location":"lab6/using-docker-compose/#step-7-viewing-the-services-on-docker-dashboard","title":"Step 7. Viewing the Services on Docker Dashboard","text":""},{"location":"lab6/using-docker-compose/#step-8-accessing-the-app","title":"Step 8. Accessing the app","text":"<p>Visit http://0.0.0.0:8502 to access the following:</p> <p></p> <p>Click \"Import\". It will take a minute or two to run the import. Most of the time is spent generating the embeddings. After or during the import you can click the link to <code>http://localhost:7474</code> and log in with username \u201cneo4j\u201d and password \u201cpassword\u201d as configured in docker compose. There, you can see an overview in the left sidebar and show some connected data by clicking on the \u201cpill\u201d with the counts.</p> <p>The data loader will import the graph using the following schema.</p> <p></p>"},{"location":"lab6/using-docker-compose/#result","title":"Result:","text":"<p>The graph schema for Stack Overflow consists of nodes representing Questions, Answers, Users, and Tags. Users are linked to Questions they\u2019ve asked via the \u201cASKED\u201d relationship and to Answers they\u2019ve provided with the \u201cANSWERS\u201d relationship. Each Answer is also inherently associated with a specific Question. Furthermore, Questions are categorized by their relevant topics or technologies using the \u201cTAGGED\u201d relationship connecting them to Tags.</p>"},{"location":"lab6/using-docker-compose/#step-9-accessing-the-neo4j","title":"Step 9. Accessing the Neo4j","text":"<p>As instructed, open <code>http://localhost:7474</code> and log in with username \u201cneo4j\u201d and password \u201cpassword\u201d as configured in docker compose.</p> <p></p>"},{"location":"lab6/using-docker-compose/#step-10-query-the-imported-data-via-a-chat-interface-using-vector-graph-search","title":"Step 10. Query the Imported Data via a Chat Interface Using Vector + Graph Search","text":"<p>This application server on <code>http://localhost:8501</code> has the classic LLM chat UI and lets the user ask questions and get answers.</p> <p>There\u2019s a switch called RAG mode where the user can rely either completely on the LLMs trained knowledge (RAG: Disabled), or the more capable (RAG: Enabled) mode where the application uses similarity search using text embedding and graph queries to find the most relevant questions and answers in the database.</p> <p>Click \"Highly ranked questions\"</p>"},{"location":"lab6/using-docker-compose/#step-11-accessing-genai-stack-pdf-bot","title":"Step 11. Accessing GenAI Stack PDF Bot","text":"<p>Open <code>http://0.0.0.0:8503/</code> on the browser to access the PDF Bot that allows you to chat with your PDF file.</p> <p></p> <p>In order to test drive, I uploaded my latest resume and asked a quick question.  It responded back with the right answer. Amazing !!</p>"},{"location":"prereq/prerequisites/","title":"Prerequisites:","text":""},{"location":"prereq/prerequisites/#1-docker-desktop","title":"1. Docker Desktop","text":"<p>Download and Install Docker Desktop on your system. Make sure you are using Docker Desktop v4.27.2 and above.</p> <ul> <li>Apple Chip</li> <li>Intel Chip</li> <li>Windows</li> <li>Linux</li> </ul>"},{"location":"prereq/prerequisites/#enabling-wsl-2-based-engine-on-docker-desktop-for-windows","title":"Enabling WSL 2 based engine on Docker Desktop for Windows","text":"<p>In case you're using Windows 11, you will need to enable WSL 2 by opening Docker Desktop &gt; Settings &gt; Resources &gt; WSL Integration</p> <p></p>"},{"location":"prereq/prerequisites/#2-ollama","title":"2. Ollama","text":"<p>Download and Install Ollama on your system.</p> <p>Note: For Windows Users, if you have antivirus installed on your system Ollama installation might be flagged as virus. It is advisable to disbale your antivirus/firewall before installation and enable it back once the installation is complete.</p>"}]}